{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class _Node:\n",
    "    is_leaf: bool=False; w:float=0.0\n",
    "    feat:int=None; thr:float=None; left=None; right=None\n",
    "\n",
    "\n",
    "class ScratchGBRegressor:\n",
    "    def __init__(self, n_estimators=200, learning_rate=0.1,\n",
    "                 max_depth=3, subsample=1.0, colsample=1.0,\n",
    "                 reg_lambda=1.0, gamma=0.0, random_state=None):\n",
    "        self.M=n_estimators; self.lr=learning_rate; self.depth=max_depth\n",
    "        self.sub=subsample; self.col=colsample; self.lam=reg_lambda\n",
    "        self.gamma=gamma; self.rng=np.random.default_rng(random_state)\n",
    "        self.trees:List[_Node]=[]; self.mean_=0; self.history_=[]\n",
    "\n",
    "    @staticmethod\n",
    "    def _grad_hess(y, y_hat):\n",
    "        g=-2*(y-y_hat); h=np.full_like(g,2.0); return g.ravel(),h.ravel()\n",
    "\n",
    "    def _best_split(self, x, g, h):\n",
    "        idx=np.argsort(x); g_c=np.cumsum(g[idx]); h_c=np.cumsum(h[idx])\n",
    "        G,H=g_c[-1],h_c[-1]; best_gain,best_thr=-np.inf,None\n",
    "        for i in range(1,len(idx)):\n",
    "            if x[idx[i]]==x[idx[i-1]]: continue\n",
    "            GL,HL=g_c[i-1],h_c[i-1]; GR,HR=G-GL,H-HL\n",
    "            gain=(GL**2)/(HL+self.lam)+(GR**2)/(HR+self.lam)-(G**2)/(H+self.lam)-self.gamma\n",
    "            if gain>best_gain: best_gain,best_thr=gain,0.5*(x[idx[i]]+x[idx[i-1]])\n",
    "        return best_gain,best_thr\n",
    "\n",
    "    def _build(self, rows, depth, X, g, h):\n",
    "        G,H=g[rows].sum(),h[rows].sum(); w=-G/(H+self.lam); node=_Node(w=w)\n",
    "        if depth==self.depth or len(rows)<2: node.is_leaf=True; return node\n",
    "        feats=self.rng.choice(X.shape[1],max(1,int(self.col*X.shape[1])),replace=False)\n",
    "        best_gain,b_f,b_t=-np.inf,None,None\n",
    "        for f in feats:\n",
    "            gain,thr=self._best_split(X[rows,f],g[rows],h[rows])\n",
    "            if gain>best_gain: best_gain,b_f,b_t=gain,f,thr\n",
    "        if best_gain<=0 or b_t is None: node.is_leaf=True; return node\n",
    "        node.feat,node.thr=b_f,b_t\n",
    "        L=rows[X[rows,b_f]<=b_t]; R=rows[X[rows,b_f]>b_t]\n",
    "        node.left =self._build(L,depth+1,X,g,h)\n",
    "        node.right=self._build(R,depth+1,X,g,h)\n",
    "        return node\n",
    "\n",
    "    def _pred_tree(self,node,X):\n",
    "        if node.is_leaf: return np.full(X.shape[0],node.w)\n",
    "        m=X[:,node.feat]<=node.thr\n",
    "        out=np.empty(X.shape[0])\n",
    "        out[m]= self._pred_tree(node.left ,X[m])\n",
    "        out[~m]=self._pred_tree(node.right,X[~m])\n",
    "        return out\n",
    "\n",
    "    def fit(self,X,y):\n",
    "        y=y.reshape(-1,1); self.mean_=y.mean(); y_hat=np.full_like(y,self.mean_)\n",
    "        for _ in range(self.M):\n",
    "            rows=self.rng.random(len(y))<self.sub\n",
    "            g,h=self._grad_hess(y,y_hat)\n",
    "            tree=self._build(np.where(rows)[0],0,X,g,h)\n",
    "            self.trees.append(tree)\n",
    "            y_hat += self.lr*self._pred_tree(tree,X).reshape(-1,1)\n",
    "            self.history_.append(mean_squared_error(y, y_hat))\n",
    "\n",
    "    def predict(self,X):\n",
    "        y=np.full(X.shape[0],self.mean_)\n",
    "        for t in self.trees: y += self.lr*self._pred_tree(t,X)\n",
    "        return y\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "X,y=make_regression(n_samples=1000,n_features=5,noise=12)\n",
    "y[np.random.choice(len(y),30,replace=False)] += np.random.normal(0,150,30)\n",
    "\n",
    "X_tr,X_te,y_tr,y_te=train_test_split(X,y,test_size=.3,random_state=0)\n",
    "\n",
    "my=ScratchGBRegressor(n_estimators=200,learning_rate=.1,max_depth=3,\n",
    "                      subsample=.8,colsample=.8,reg_lambda=1.,random_state=0)\n",
    "my.fit(X_tr,y_tr)\n",
    "\n",
    "xgb=XGBRegressor(n_estimators=200,learning_rate=.1,max_depth=3,\n",
    "                 subsample=.8,colsample_bytree=.8,reg_lambda=1.,\n",
    "                 tree_method=\"exact\",objective=\"reg:squarederror\",\n",
    "                 random_state=0,verbosity=0)\n",
    "xgb.fit(X_tr,y_tr,eval_set=[(X_tr,y_tr)],eval_metric=\"rmse\",verbose=False)\n",
    "xgb_hist=np.square(xgb.evals_result()['validation_0']['rmse'])  # MSE = RMSE²\n",
    "\n",
    "# mse curve\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.plot(my.history_, label='ScratchGB – train MSE')\n",
    "plt.plot(xgb_hist,       label='XGBoost   – train MSE')\n",
    "plt.xlabel('Iteration'); plt.ylabel('MSE'); plt.title('Convergence')\n",
    "plt.legend(); plt.grid(); plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
